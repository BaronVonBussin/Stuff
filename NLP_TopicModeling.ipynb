{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNU0zqyfGne8RfwDTbWqGF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaronVonBussin/Stuff/blob/main/NLP_TopicModeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8RYV19U0Lln",
        "outputId": "caafd81e-a0e1-41e1-9aca-b029a9d4acee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy==3.* in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (2.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.*) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.*) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.*) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.*) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.*) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.*) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.*) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.*) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.*) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy==3.*) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy==3.*) (2.1.3)\n",
            "2023-08-18 15:10:14.505468: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2023-08-18 15:10:32.469566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.6.1                         \n",
            "Location         /usr/local/lib/python3.10/dist-packages/spacy\n",
            "Platform         Linux-5.15.109+-x86_64-with-glibc2.35\n",
            "Python version   3.10.12                       \n",
            "Pipelines        en_core_web_sm (3.6.0)        \n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy==3.*\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy info"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upgrade gensim in case.\n",
        "# !pip install --upgrade numpy\n",
        "!pip install -U gensim==4.*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv8HcxmO0Mmh",
        "outputId": "ff89d31e-1fab-440c-f0be-dec80471c289"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.* in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim==4.*) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim==4.*) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.*) (6.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import spacy\n",
        "\n",
        "from gensim import models, corpora\n",
        "from gensim import similarities\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "jPbV7u2s0OqR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "SVhw6U1G0P_B"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4-fcaxt0RgR",
        "outputId": "ae1f4d0b-2da0-430a-c062-b18e1a906f35"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the CNN corpus.\n",
        "!gdown 'https://drive.google.com/uc?id=122fC9XpNwFKx0ryRVKJz5MWUTzA3Vpsf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQF9dQiF0SzD",
        "outputId": "843753a3-9d69-4cbf-d2dd-154e65118e15"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (uriginal): https://drive.google.com/uc?id=122fC9XpNwFKx0ryRVKJz5MWUTzA3Vpsf\n",
            "From (redirected): https://drive.google.com/uc?id=122fC9XpNwFKx0ryRVKJz5MWUTzA3Vpsf&confirm=t&uuid=729f40a6-b8d5-4432-8822-73b11ea89bc8\n",
            "To: /content/cnn_articles.txt\n",
            "100% 365M/365M [00:02<00:00, 132MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('cnn_articles.txt', 'r', encoding='utf8') as f:\n",
        "  articles = f.read().split('@delimiter')"
      ],
      "metadata": {
        "id": "PFzvK_Q-0Tuh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(articles))\n",
        "print(articles[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Mw90NoZ0Uor",
        "outputId": "9121c1a2-5299-4586-96aa-6fb57ca82d8b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92579\n",
            " -- Children in war-ravaged Afghanistan are safer than those growing up in London or New York, NATO's top civilian envoy says.\n",
            "\n",
            "Mark Sedwill, the senior civilian representative for NATO, made the comments on an episode of CBBC's \"Newsround,\" which is airing Monday.\n",
            "\n",
            "In the show -- a BBC current-affairs program for children -- several youngsters in Kabul, Afghanistan, say they are afraid of daily violence and the frequent explosions in their war-torn country.\n",
            "\n",
            "In response, Sedwill says: \"Here in Kabul, and other big cities actually, there are very few of these bombs. The children are probably safer here than they would be in London, New York or Glasgow or many other cities.\n",
            "\n",
            "\"Most children can go about their lives in safety. It's a very family-oriented society. So it is a little bit like a city of villages,\" he added.\n",
            "\n",
            "A U.N. report released earlier this year seems to contradict Sedwill's assessment.\n",
            "\n",
            "The February report, by the special representative for children and armed conflict, says that children in Afghanistan are increasingly the victims of roadside attacks, crossfire between militants and international forces, and air strikes.\n",
            "\n",
            "In addition, they have been used as human shields and recruited as suicide bombers.\n",
            "\n",
            "The report also said boys are sexually abused and exploited by armed groups and girls schools have been burned down.\n",
            "\n",
            "Earlier this year, there was a rash of poisonings involving schoolgirls, and several instances of acid attacks have been reported.\n",
            "\n",
            "On Monday, Sedwill elaborated on what he meant in his remark.\n",
            "\n",
            "\"Any comment you have to clarify obviously wasn't very well put and the comparison I made with western cities distracted attention from the important point I was seeking to make,\" he said.\n",
            "\n",
            "\"I was trying to explain to an audience of British children how uneven violence is across Afghanistan. Half the insurgent violence takes place in 10 of the 365 districts, and, in those places, children are too often the victims of IEDs and other dangers. But, in cities like Kabul where security has improved, the total levels of violence, including criminal violence, are comparable to those which many western children would experience.\n",
            "\n",
            "\"For most Afghans, the biggest challenges are from poverty -- the absence of clean water, open sewers, malnutrition, disease -- and many more children are at risk from those problems than from the insurgency.\"\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_SIZE = 20000\n",
        "dataset = articles[:DATASET_SIZE]"
      ],
      "metadata": {
        "id": "6-bqGNFq0Ve7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank('en')\n",
        "\n",
        "def basic_filter(tokenized_doc):\n",
        "  return [t.text for t in tokenized_doc if\n",
        "          not t.is_punct and \\\n",
        "          not t.is_space and \\\n",
        "          t.is_alpha]"
      ],
      "metadata": {
        "id": "FbUb5UKY0WoZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_PROCESS = 4"
      ],
      "metadata": {
        "id": "TPUJZe_I0X05"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tokenized_articles = list(map(basic_filter, nlp.pipe(dataset, n_process=NUM_PROCESS)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8r6S5wZ0ZKZ",
        "outputId": "ef9b1f9b-7d26-4967-b7e8-8eed927ba830"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 22s, sys: 2.58 s, total: 2min 24s\n",
            "Wall time: 3min 10s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_articles[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It3kmzeh0aqM",
        "outputId": "f78884ba-8a17-4b8e-a86a-7efde277c1f4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Children', 'in', 'war', 'ravaged', 'Afghanistan', 'are', 'safer', 'than', 'those', 'growing', 'up', 'in', 'London', 'or', 'New', 'York', 'NATO', 'top', 'civilian', 'envoy', 'says', 'Mark', 'Sedwill', 'the', 'senior', 'civilian', 'representative', 'for', 'NATO', 'made', 'the', 'comments', 'on', 'an', 'episode', 'of', 'CBBC', 'Newsround', 'which', 'is', 'airing', 'Monday', 'In', 'the', 'show', 'a', 'BBC', 'current', 'affairs', 'program', 'for', 'children', 'several', 'youngsters', 'in', 'Kabul', 'Afghanistan', 'say', 'they', 'are', 'afraid', 'of', 'daily', 'violence', 'and', 'the', 'frequent', 'explosions', 'in', 'their', 'war', 'torn', 'country', 'In', 'response', 'Sedwill', 'says', 'Here', 'in', 'Kabul', 'and', 'other', 'big', 'cities', 'actually', 'there', 'are', 'very', 'few', 'of', 'these', 'bombs', 'The', 'children', 'are', 'probably', 'safer', 'here', 'than', 'they', 'would', 'be', 'in', 'London', 'New', 'York', 'or', 'Glasgow', 'or', 'many', 'other', 'cities', 'Most', 'children', 'can', 'go', 'about', 'their', 'lives', 'in', 'safety', 'It', 'a', 'very', 'family', 'oriented', 'society', 'So', 'it', 'is', 'a', 'little', 'bit', 'like', 'a', 'city', 'of', 'villages', 'he', 'added', 'A', 'report', 'released', 'earlier', 'this', 'year', 'seems', 'to', 'contradict', 'Sedwill', 'assessment', 'The', 'February', 'report', 'by', 'the', 'special', 'representative', 'for', 'children', 'and', 'armed', 'conflict', 'says', 'that', 'children', 'in', 'Afghanistan', 'are', 'increasingly', 'the', 'victims', 'of', 'roadside', 'attacks', 'crossfire', 'between', 'militants', 'and', 'international', 'forces', 'and', 'air', 'strikes', 'In', 'addition', 'they', 'have', 'been', 'used', 'as', 'human', 'shields', 'and', 'recruited', 'as', 'suicide', 'bombers', 'The', 'report', 'also', 'said', 'boys', 'are', 'sexually', 'abused', 'and', 'exploited', 'by', 'armed', 'groups', 'and', 'girls', 'schools', 'have', 'been', 'burned', 'down', 'Earlier', 'this', 'year', 'there', 'was', 'a', 'rash', 'of', 'poisonings', 'involving', 'schoolgirls', 'and', 'several', 'instances', 'of', 'acid', 'attacks', 'have', 'been', 'reported', 'On', 'Monday', 'Sedwill', 'elaborated', 'on', 'what', 'he', 'meant', 'in', 'his', 'remark', 'Any', 'comment', 'you', 'have', 'to', 'clarify', 'obviously', 'was', 'very', 'well', 'put', 'and', 'the', 'comparison', 'I', 'made', 'with', 'western', 'cities', 'distracted', 'attention', 'from', 'the', 'important', 'point', 'I', 'was', 'seeking', 'to', 'make', 'he', 'said', 'I', 'was', 'trying', 'to', 'explain', 'to', 'an', 'audience', 'of', 'British', 'children', 'how', 'uneven', 'violence', 'is', 'across', 'Afghanistan', 'Half', 'the', 'insurgent', 'violence', 'takes', 'place', 'in', 'of', 'the', 'districts', 'and', 'in', 'those', 'places', 'children', 'are', 'too', 'often', 'the', 'victims', 'of', 'IEDs', 'and', 'other', 'dangers', 'But', 'in', 'cities', 'like', 'Kabul', 'where', 'security', 'has', 'improved', 'the', 'total', 'levels', 'of', 'violence', 'including', 'criminal', 'violence', 'are', 'comparable', 'to', 'those', 'which', 'many', 'western', 'children', 'would', 'experience', 'For', 'most', 'Afghans', 'the', 'biggest', 'challenges', 'are', 'from', 'poverty', 'the', 'absence', 'of', 'clean', 'water', 'open', 'sewers', 'malnutrition', 'disease', 'and', 'many', 'more', 'children', 'are', 'at', 'risk', 'from', 'those', 'problems', 'than', 'from', 'the', 'insurgency']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_TOPICS = 20"
      ],
      "metadata": {
        "id": "EW-7ZOja0bgg"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a Dictionary of word<-->id mappings.\n",
        "%%time\n",
        "dictionary = corpora.Dictionary(tokenized_articles)\n",
        "\n",
        "sample_token = 'news'\n",
        "print(f'Id for \\'{sample_token}\\' token: {dictionary.token2id[sample_token]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v72HPJpZ0chh",
        "outputId": "6dc84286-5833-483f-b601-597bec99313c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Id for 'news' token: 1039\n",
            "CPU times: user 12.2 s, sys: 26 ms, total: 12.2 s\n",
            "Wall time: 12.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "corpus_bow = [dictionary.doc2bow(article) for article in tokenized_articles]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UydQfU4r0eBZ",
        "outputId": "a28ba632-1864-4453-e640-94cfffa7619c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7.14 s, sys: 23.9 ms, total: 7.17 s\n",
            "Wall time: 7.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lda_model = models.LdaModel(corpus=corpus_bow, num_topics=NUM_TOPICS, id2word=dictionary, random_state=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deiRakrp0fID",
        "outputId": "db0f5ec0-3224-444c-a351-1ab715710db0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 54.6 s, sys: 38.9 s, total: 1min 33s\n",
            "Wall time: 55 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.print_topics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swXma_HL0hR5",
        "outputId": "79b3d6e1-f505-4917-80c0-ac191ffc679d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.068*\"the\" + 0.041*\"of\" + 0.027*\"and\" + 0.025*\"to\" + 0.025*\"in\" + 0.023*\"a\" + 0.013*\"is\" + 0.009*\"for\" + 0.008*\"that\" + 0.008*\"The\"'),\n",
              " (1,\n",
              "  '0.046*\"the\" + 0.031*\"I\" + 0.030*\"a\" + 0.028*\"to\" + 0.022*\"and\" + 0.019*\"in\" + 0.017*\"of\" + 0.012*\"was\" + 0.012*\"it\" + 0.012*\"that\"'),\n",
              " (2,\n",
              "  '0.064*\"the\" + 0.030*\"of\" + 0.025*\"to\" + 0.023*\"and\" + 0.020*\"in\" + 0.017*\"a\" + 0.011*\"that\" + 0.011*\"is\" + 0.009*\"Syria\" + 0.008*\"for\"'),\n",
              " (3,\n",
              "  '0.038*\"the\" + 0.032*\"to\" + 0.029*\"and\" + 0.029*\"of\" + 0.024*\"a\" + 0.017*\"that\" + 0.016*\"in\" + 0.013*\"is\" + 0.010*\"are\" + 0.009*\"for\"'),\n",
              " (4,\n",
              "  '0.069*\"the\" + 0.030*\"of\" + 0.029*\"to\" + 0.024*\"and\" + 0.021*\"in\" + 0.019*\"a\" + 0.015*\"said\" + 0.011*\"The\" + 0.010*\"that\" + 0.009*\"on\"'),\n",
              " (5,\n",
              "  '0.060*\"the\" + 0.031*\"in\" + 0.029*\"of\" + 0.028*\"to\" + 0.027*\"and\" + 0.021*\"a\" + 0.014*\"said\" + 0.011*\"that\" + 0.011*\"The\" + 0.008*\"is\"'),\n",
              " (6,\n",
              "  '0.054*\"the\" + 0.033*\"to\" + 0.030*\"of\" + 0.027*\"in\" + 0.026*\"and\" + 0.025*\"a\" + 0.012*\"said\" + 0.011*\"that\" + 0.009*\"The\" + 0.009*\"for\"'),\n",
              " (7,\n",
              "  '0.065*\"the\" + 0.028*\"to\" + 0.028*\"in\" + 0.027*\"of\" + 0.025*\"and\" + 0.018*\"a\" + 0.011*\"for\" + 0.010*\"said\" + 0.009*\"The\" + 0.008*\"that\"'),\n",
              " (8,\n",
              "  '0.062*\"the\" + 0.033*\"to\" + 0.026*\"of\" + 0.022*\"a\" + 0.020*\"and\" + 0.018*\"in\" + 0.015*\"that\" + 0.011*\"for\" + 0.010*\"is\" + 0.010*\"on\"'),\n",
              " (9,\n",
              "  '0.051*\"the\" + 0.029*\"and\" + 0.027*\"of\" + 0.025*\"to\" + 0.021*\"a\" + 0.018*\"in\" + 0.011*\"I\" + 0.010*\"for\" + 0.010*\"was\" + 0.010*\"that\"'),\n",
              " (10,\n",
              "  '0.055*\"the\" + 0.026*\"to\" + 0.022*\"of\" + 0.021*\"and\" + 0.021*\"in\" + 0.020*\"a\" + 0.014*\"for\" + 0.009*\"that\" + 0.009*\"is\" + 0.008*\"on\"'),\n",
              " (11,\n",
              "  '0.033*\"the\" + 0.024*\"News\" + 0.023*\"to\" + 0.023*\"a\" + 0.021*\"of\" + 0.021*\"CNN\" + 0.017*\"you\" + 0.017*\"Student\" + 0.013*\"and\" + 0.013*\"is\"'),\n",
              " (12,\n",
              "  '0.068*\"her\" + 0.054*\"she\" + 0.031*\"the\" + 0.030*\"to\" + 0.022*\"a\" + 0.020*\"said\" + 0.020*\"was\" + 0.018*\"and\" + 0.017*\"She\" + 0.015*\"of\"'),\n",
              " (13,\n",
              "  '0.058*\"the\" + 0.026*\"to\" + 0.026*\"and\" + 0.017*\"of\" + 0.016*\"in\" + 0.015*\"a\" + 0.011*\"is\" + 0.010*\"for\" + 0.007*\"The\" + 0.007*\"with\"'),\n",
              " (14,\n",
              "  '0.052*\"the\" + 0.029*\"to\" + 0.027*\"a\" + 0.027*\"and\" + 0.025*\"of\" + 0.016*\"in\" + 0.013*\"is\" + 0.011*\"for\" + 0.010*\"that\" + 0.009*\"on\"'),\n",
              " (15,\n",
              "  '0.001*\"the\" + 0.001*\"a\" + 0.001*\"to\" + 0.001*\"and\" + 0.000*\"of\" + 0.000*\"in\" + 0.000*\"that\" + 0.000*\"is\" + 0.000*\"he\" + 0.000*\"for\"'),\n",
              " (16,\n",
              "  '0.063*\"the\" + 0.030*\"in\" + 0.029*\"a\" + 0.026*\"to\" + 0.021*\"of\" + 0.017*\"and\" + 0.011*\"on\" + 0.011*\"with\" + 0.010*\"was\" + 0.009*\"for\"'),\n",
              " (17,\n",
              "  '0.057*\"the\" + 0.029*\"a\" + 0.024*\"and\" + 0.024*\"to\" + 0.022*\"said\" + 0.022*\"was\" + 0.021*\"of\" + 0.020*\"in\" + 0.014*\"he\" + 0.013*\"his\"'),\n",
              " (18,\n",
              "  '0.096*\"million\" + 0.017*\"weekend\" + 0.010*\"percent\" + 0.010*\"earned\" + 0.009*\"office\" + 0.009*\"total\" + 0.009*\"box\" + 0.008*\"film\" + 0.008*\"theaters\" + 0.008*\"the\"'),\n",
              " (19,\n",
              "  '0.041*\"the\" + 0.033*\"he\" + 0.030*\"to\" + 0.021*\"his\" + 0.021*\"a\" + 0.020*\"in\" + 0.020*\"that\" + 0.018*\"of\" + 0.016*\"and\" + 0.013*\"was\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "def improved_filter(tokenized_doc):\n",
        "  return [t.lemma_ for t in tokenized_doc if\n",
        "          t.is_alpha and \\\n",
        "          not t.is_punct and \\\n",
        "          not t.is_space and \\\n",
        "          not t.is_stop and \\\n",
        "          t.pos_ in ['NOUN', 'VERB', 'ADJ']]"
      ],
      "metadata": {
        "id": "gTuYQoFf0iQx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll need to retokenize everything and rebuild the BOWs. Because we're now\n",
        "# using the POS tagger, this will take longer. The \"w_pos\" in the variable\n",
        "# names below just means \"with part-of-speech\".\n",
        "%%time\n",
        "tokenized_articles_w_pos = list(map(improved_filter, nlp.pipe(dataset, n_process=NUM_PROCESS)))\n",
        "dictionary_w_pos = corpora.Dictionary(tokenized_articles_w_pos)\n",
        "corpus_bow_w_pos = [dictionary_w_pos.doc2bow(article) for article in tokenized_articles_w_pos]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "hhczTIqM0j4I",
        "outputId": "b53f3240-dded-4fb9-e488-a4d59771791b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[1;32m   1609\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpipes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m                 \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m_multiprocessing_pipe\u001b[0;34m(self, texts, pipes, n_process, batch_size)\u001b[0m\n\u001b[1;32m   1683\u001b[0m         )\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m             for i, (_, (byte_doc, context, byte_error)) in enumerate(\n\u001b[0m\u001b[1;32m   1686\u001b[0m                 \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyte_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m             ):\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1680\u001b[0m         \u001b[0;31m# The received object is a batch of byte-encoded docs, so flatten them with chain.from_iterable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m         byte_tuples = chain.from_iterable(\n\u001b[0;32m-> 1682\u001b[0;31m             \u001b[0mrecv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrecv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytedocs_recv_ch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m         )\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lda_model = models.LdaModel(corpus=corpus_bow_w_pos, num_topics=NUM_TOPICS, id2word=dictionary_w_pos, random_state=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "a2_P5oh90lsx",
        "outputId": "0e4008bf-ae4a-4a8d-e48e-495bfd9db711"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'corpus_bow_w_pos' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.print_topics()"
      ],
      "metadata": {
        "id": "IOWp_Qjz0mzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The size of the dictionary before filtering.\n",
        "len(dictionary_w_pos)"
      ],
      "metadata": {
        "id": "W8JEGovz0otJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary_w_pos.filter_extremes(no_below=5, no_above=0.5)"
      ],
      "metadata": {
        "id": "4_6GumHJ0qLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The size of the dictionary after filtering.\n",
        "len(dictionary_w_pos)"
      ],
      "metadata": {
        "id": "1AqB96zK0rLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild bag of words.\n",
        "corpus_bow_w_pos_filtered = [dictionary_w_pos.doc2bow(article) for article in tokenized_articles_w_pos]"
      ],
      "metadata": {
        "id": "Xw-ZJhMW0sCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lda_model = models.ldamodel.LdaModel(corpus=corpus_bow_w_pos_filtered,\n",
        "                                     id2word=dictionary_w_pos,\n",
        "                                     num_topics=NUM_TOPICS,\n",
        "                                     passes=10,\n",
        "                                     alpha='auto',\n",
        "                                     eta='auto',\n",
        "                                     random_state=1)"
      ],
      "metadata": {
        "id": "xYC1T0kb0tcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.print_topics()"
      ],
      "metadata": {
        "id": "j-Z6X0UH0vDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lda_model.alpha)\n",
        "print(lda_model.eta)"
      ],
      "metadata": {
        "id": "Uge5IEM40v9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_idx = 0\n",
        "print(dataset[article_idx][:300])"
      ],
      "metadata": {
        "id": "4b8YldUc0xAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return topic distribution for an article sorted by probability.\n",
        "topics = sorted(lda_model.get_document_topics(corpus_bow_w_pos_filtered[article_idx]), key=lambda tup: tup[1])[::-1]\n",
        "topics"
      ],
      "metadata": {
        "id": "J9p1SqL40yJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the words of the top topic from the previous article.\n",
        "lda_model.show_topic(topics[0][0])"
      ],
      "metadata": {
        "id": "R8AeCiP00zQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the words of the second-most prevalent topic from the previous article.\n",
        "lda_model.show_topic(topics[1][0])"
      ],
      "metadata": {
        "id": "Zhdc8WsF00ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_topics(article_idx, min_topic_prob):\n",
        "\n",
        "  # Sort from highest to lowest topic probability.\n",
        "  topic_prob_pairs = sorted(lda_model.get_document_topics(corpus_bow_w_pos_filtered[article_idx],\n",
        "                                                          minimum_probability=min_topic_prob),\n",
        "                            key=lambda tup: tup[1])[::-1]\n",
        "\n",
        "  word_prob_pairs = [lda_model.show_topic(pair[0]) for pair in topic_prob_pairs]\n",
        "  topic_words = [[pair[0] for pair in collection] for collection in word_prob_pairs]\n",
        "\n",
        "  data = {\n",
        "      'Major Topics': topic_prob_pairs,\n",
        "      'Topic Words': topic_words\n",
        "  }\n",
        "\n",
        "  return pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "CkArimve01ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('max_colwidth', 600)\n",
        "snippet_length = 300\n",
        "min_topic_prob = 0.25\n",
        "\n",
        "article_idx = 1\n",
        "print(dataset[article_idx][:snippet_length])\n",
        "get_top_topics(article_idx, min_topic_prob)"
      ],
      "metadata": {
        "id": "5IHKFtme03Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_idx = 10\n",
        "print(dataset[article_idx][:snippet_length])\n",
        "get_top_topics(article_idx, min_topic_prob)"
      ],
      "metadata": {
        "id": "lVCmZRRq04PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_idx = 100\n",
        "print(dataset[article_idx][:snippet_length])\n",
        "get_top_topics(article_idx, min_topic_prob)"
      ],
      "metadata": {
        "id": "CJNPbBuf05jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_idx = 1000\n",
        "print(dataset[article_idx][:snippet_length])\n",
        "get_top_topics(article_idx, min_topic_prob)"
      ],
      "metadata": {
        "id": "0PalxCzZ06pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_idx = 10000\n",
        "print(dataset[article_idx][:snippet_length])\n",
        "get_top_topics(article_idx, 0.25)"
      ],
      "metadata": {
        "id": "BZ3nKbXK07nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_articles_w_pos, dictionary=dictionary_w_pos, coherence='u_mass')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "id": "gT9Oc5H608eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_word_cloud(model, rows, cols, max_words):\n",
        "  word_cloud = WordCloud(background_color='white', max_words=max_words, prefer_horizontal=1.0)\n",
        "  fig, axes = plt.subplots(rows, cols, figsize=(15,15))\n",
        "\n",
        "  for i, ax in enumerate(axes.flatten()):\n",
        "      fig.add_subplot(ax)\n",
        "      topic_words = dict(model.show_topic(i))\n",
        "      word_cloud.generate_from_frequencies(topic_words)\n",
        "      plt.gca().imshow(word_cloud, interpolation='bilinear')\n",
        "      plt.gca().set_title('Topic {id}'.format(id=i))\n",
        "      plt.gca().axis('off')\n",
        "\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "C9mqT91x09uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we'll visualize the first nine topics.\n",
        "render_word_cloud(lda_model, 3, 3, 10)"
      ],
      "metadata": {
        "id": "TojBBnEd0_DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_index = similarities.MatrixSimilarity(lda_model[corpus_bow_w_pos_filtered], num_features=len(dictionary_w_pos))"
      ],
      "metadata": {
        "id": "gBz1N_Vk1AIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_articles(index, model, article_bow, top_n=5, first_m_words=300):\n",
        "  # model[article_bow] retrieves the topic distribution for the BOW.\n",
        "  # index[model[article_bow] compares the topic distribution for the BOW against the similarity index previously computed.\n",
        "  similar_docs = index[model[article_bow]]\n",
        "  top_n_docs = sorted(enumerate(similar_docs), key=lambda item: -item[1])[1:top_n+1]\n",
        "\n",
        "  # Return a list of tuples with each tuple: (article id, similarity score, first_m_words of article)\n",
        "  return list(map(lambda entry: (entry[0], entry[1], articles[entry[0]][:first_m_words]), top_n_docs))"
      ],
      "metadata": {
        "id": "6bnN-wIJ1Bly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_idx = 0\n",
        "print(dataset[article_idx][:snippet_length], '\\n')\n",
        "get_similar_articles(lda_index, lda_model, corpus_bow_w_pos_filtered[article_idx])"
      ],
      "metadata": {
        "id": "hjD6EUfs1C_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_idx = 10\n",
        "print(dataset[article_idx][:snippet_length], '\\n')\n",
        "get_similar_articles(lda_index, lda_model, corpus_bow_w_pos_filtered[article_idx])"
      ],
      "metadata": {
        "id": "F_wQAM0q1ERT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_idx = 100\n",
        "print(dataset[article_idx][:snippet_length], '\\n')\n",
        "get_similar_articles(lda_index, lda_model, corpus_bow_w_pos_filtered[article_idx])"
      ],
      "metadata": {
        "id": "V_UpU24F1FNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_article = \"Capricorn Business Acquisitions Inc. (TSXV: CAK.H) (the “Company“) is pleased to announce that its board has approved the issuance of 70,000 stock options (“Stock Options“) to directors on April 19, 2020.\"\n",
        "\n",
        "article_tokens = list(map(improved_filter, [nlp(test_article)]))[0]\n",
        "article_bow = dictionary_w_pos.doc2bow(article_tokens)\n",
        "get_similar_articles(lda_index, lda_model, article_bow)"
      ],
      "metadata": {
        "id": "gZFNsyRX1GBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_article = \"DEA agent sentenced to 12 years in prison for conspiring with Colombian drug cartel.\"\n",
        "\n",
        "article_tokens = list(map(improved_filter, [nlp(test_article)]))[0]\n",
        "article_bow = dictionary_w_pos.doc2bow(article_tokens)\n",
        "get_similar_articles(lda_index, lda_model, article_bow)"
      ],
      "metadata": {
        "id": "Ti4Nc9S41HWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W17d9wZ91ITJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}